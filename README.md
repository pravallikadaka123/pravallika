building a real-time data streaming pipeline, covering each phase from data ingestion to processing and finally storage.
We'll utilize a powerful stack of tools and technologies, including Apache Airflow, Python, Apache Kafka, Apache Zookeeper, Apache Spark, and Cassandraâ€”all neatly containerized using Docker.
0:53 System architecture
3:47 Getting data from API with Airflow
17:10 Docker Compose for the architecture
26:09 Streaming data into Kafka
44:29 Apache Spark and Cassandra setup
49:33 Streaming data into cassandra

entrypoint.sh---->we want to write sequence of command that airflow should follow when its trying to initialize web server or the scheduler
